%       File: VTthesis_template.tex
%     Created: Thu Mar 24 11:00 AM 2016 EDT
%     Last Change: 04/19/2021
%     Author: Chandler Jearls, VT
%
% This template is designed to operate with XeLaTeX.
%
% All elements in the Title, Abstract, and Keywords MUST be formatted as text and NOT as math.
%
%Further instructions for using this template are embedded in the document. Additionally, there are comments at the end of the file that give suggestions on writing your thesis.  
%
%In addition to the standard formatting options, the following options are defined for the VTthesis class: proposal, prelim, doublespace, draft. 

\documentclass[doublespace,draft,nopageskip]{VTthesis} % nopageskip - Removes arbitrary blank pages.

% Using the following header instead will create a draft copy of your thesis
%\documentclass[doublespace,draft]{VTthesis}

% Title of your thesis
\title{Open-Source Parameterized Low-Latency Aggressive Hardware Compressor and Decompressor for Memory Compression}

% You should include 3-5 keywords, separated by commas
\keywords{compression, decompression, accelerator, hardware, parameterized, low-latency}

% Your name, including middle initial(s)
\author{James C. Jearls}

% Change this to your program, e.g. Physics, Civil Engineering, etc.
\program{Computer Engineering} 

% Change this to your degree, e.g. Master of Science, Master of Art, etc.
\degree{Master of Science} 

% This should be your defense date:
\submitdate{May 11, 2021} 

% Committee members. Only have five readers and one chair available.
% Only use the ones you need and don't include the ones you don't need.
% You can also declare a Co-advisor. If you do, the principal and co-advisors
% will be listed as co-advisors on the title page.  Per the VT ETD standards, 
% you should not include titles or educational qualifications such as PhD or Dr.
% You should, however, include middle initials if possible.
\principaladvisor{Ali Butt}
\firstreader{Kirk W. Cameron}
\secondreader{Xun Jian}
\thirdreader{Chang Woo Min}

% The dedication and acknowledgement pages are optional. Comment them out to remove them.
\dedication{This is where you put your dedications.}
\acknowledge{This is where you put your acknowledgments.}

% The abstract is required.
\abstract{In recent years, memory has shown to be a constraining factor in many workloads. Memory is an expensive necessity in many situations, from embedded devices with a few kilobytes of SRAM to warehouse-scale computers with thousands of terabytes of DRAM. Memory compression has existed in all major operating systems for many years. However, while faster than swapping to a disk, memory compression adds latency to data read operations. Companies and research groups have investigated hardware compression to mitigate these problems. Still, open-source low-latency hardware compressors and decompressors do not exist; as such, every group that studies hardware compression is forced to implement it. Importantly, because the devices that can benefit from memory compression vary so widely, there is no single solution to address all devices' area, latency, power, and bandwidth requirements. This work intends to address the many issues with hardware compressors and decompressors. This work implements hardware accelerators for three popular compression algorithms, LZ77, LZW, and Huffman encoding. Each implementation includes a compressor and decompressor, and all designs are entirely parameterized. All of the designs are open-source under a permissive license. Finally, configurations of the work can achieve decompression latencies under one microsecond and compression ratios comparable to software compression algorithms. {\color{red} replace this with average latencies from simulation} and compression ratios comparable to software compression algorithms.}

% The general audience abstract is required. There are currently no word limits.
%You are also required as of Spring 2016 to include a general audience abstract. This should be geared towards individuals outside of your field that may be reading seeking information about your work. You should avoid language that is particular to your field and clearly define any terms that may have special meaning in your discipline.
\abstractgenaud{Computer memory, the fast, temporary storage where programs and data are held, is expensive and limited. Compression allows for data and programs to be held in memory in a smaller format so less space is used. This work implements a hardware design for compression and decompression accelerators to make it faster for the programs using the compressed data to access it. This work includes three hardware compressor and decompressor designs that can be easily modified and are free for anyone to use however they would like.}

\begin{document}
% The following lines set up the front matter of your thesis or dissertation and are required to ensure proper formatting per the VT ETD standards. 
  \frontmatter
  \maketitle
  \tableofcontents

% The list of figures and tables are now optional per the official ETD standards.  Unless you have a very good reason for removing them, you should leave these lists in the document. Comment them out to remove them.
	\listoffigures
	\listoftables
    \printnomenclature %Creates a list of abbreviations. Comment out to remove it. 

% sample text for abbreviations:

IoT is a class of objects that incorporate computers and electronics to allow them to network with other devices and operate or record data without human input.

\nomenclature{IoT}{Internet of Things}

LZ is a class of compression algorithms based on papers by Abraham Lempel and Jacob Ziv.
 
\nomenclature{LZ}{Lempel-Ziv}
 
LZW is a number of modifications to the 1978 LZ compression algorithm made by Terry Welch.
 
\nomenclature{LZW}{Lempel-Ziv-Welch}

% The following sets up the document for the main part of the thesis or dissertation. Do not comment out or remove this line.
\mainmatter

	%now go ahead and start writing your thesis
\chapter{Introduction} \label{ch:introduction}
From small IoT microcontrollers to warehouse-scale computers, memory is a valuable but limited resource in all modern computing devices. Memory compression is a technique that can enable significantly more effective memory usage by representing data in a more compact format. This allows for less physical RAM to be used for a task, and it can improve performance in memory-bound applications by reducing the time spent swapping to a disk. Additionally, RAM requires a constant power draw to refresh its data, so using less physical RAM has the potential to lower power costs.

Software RAM compression has existed for many years in all of the major operating systems. However, software compression adds several thousand cycles of latency to memory read operations{\color{red} get an exact average latency}. This additional latency can have a significant impact on performance {\color{red} It would be good to have an example of how much read latency affects performance when swapping}. This latency can be significantly reduced if compression and decompression are hardware-accelerated.

There are several existing implementations of hardware-accelerated compressors and decompressors, but these are not suitable for memory compression research \cite{ibm,microsoft}. Most of these designs are optimized for bandwidth, resulting in poor latency {\color{red} refer to specific designs we've sited, not "most"}. They are also often very specialized for tasks other than memory compression, which could result in lower effective memory available. These designs are also almost exclusively proprietary and closed-source, so hardware memory compression research always requires building entirely new hardware before it can be evaluated.

The closed-source nature of existing works makes it difficult to conduct memory compression research for a number of reasons. Most importantly, because no state-of-the-art open-source hardware compressors and decompressors exist, researchers must spend months, if not years, designing, implementing, testing, debugging, synthesizing, and optimizing the compression hardware before it can be used for its intended task {\color{red} note the specific intended tasks that would benefit from having a pre-made design, whether it is having a power estimate, or having a latency measurement, or a power measurement}. {\color{red} the following is a complicated benefit, so it probably shouldn't be discussed here in the introduction}{\color{orange}In addition, a lack of open-source state-of-the-art designs means that comparison with the existing designs is unnecessarily difficult. For a fair comparison, the compression inputs for existing designs need to be replicated for new designs. Comparing hardware characteristics is worse, because proprietary designs cannot be re-implemented on the same manufacturing node as a new design to get relative power, frequency, and area measurements.}{\color{yellow} maybe add something in here about how a lot of the designs are FPGA designs, to it makes it even more difficult to compare them to an ASIC because of the differences in area, LUT-usage, and frequency}

Existing designs are also very specialized. Applications with different requirements will necessitate different hardware implementations. A low-power microcontroller will require a much different compressor and decompressor than a server with hundreds of CPU cores. To address this need, either many hardware compressors and decompressors must be available, or designs should be highly parameterized. Parameterization allows for fast customization and exploration of the design space, and it allows a single hardware generator to fulfill the needs of many different use cases.

This work attempts to address the shortcomings of existing hardware compressors and decompressors for use in operating system memory compression. For this work, three hardware compressor and decompressor pairs were implemented and tested. All implementations are open-source under a permissive license, allowing them to be used and modified as needed by any individual, research group, or enterprise. {\color{red} emphasize this, it seems like it might be a sloppy overexaggeration, but the entire design is completely parameterized, there are no magic numbers} Everything in the design is parameterized, from the amount of data compressed by the accelerators to the number of bits in a byte, allowing for complete customizability of the design. The compressors and decompressors can be used alone for lower area and power consumption, or together to increase the compression ratio of the data being compressed. The compressors and decompressors are also able to be configured with sub-microsecond decompression latency, even while achieving compression ratios that are comparable to those of software implementations. All of these characteristics mean that this work improves the state of the art for hardware-accelerated memory compression.

{\color{red} the thing that is really missing from the Introduction is a finding, what was the result of my work as far as latency, area, and power? A good way to validate this is to compare it to existing memory costs, so we should use paging SSDs or NUMA node latency of 150ns to 300ns as proof that the latency added by our design is acceptable}
	
%Copy/paste the code below to add sections and subsections to each chapter. Add your own text to the chapter and (sub)section labels to create custom headings.          
%     \section{One Section} \label{se:one_section}
% 			\subsection{A sub-section} \label{ss:this_subsection}
% 		\section{Another Section} \label{se:another_section}

\chapter{Literature Review} \label{ch:literature_review}
\section{Compression Algorithms}\label{se:compression_algorithms}
There are two primary types of lossless compression algorithms: those that alter character encoding methods and those that match patterns of characters. Although there are many examples of both compression techniques, this work focuses on Huffman encoding for character compression and LZ-based strategies for pattern compression. This is because they are the most common bases in the state-of-the-art for memory compression.

\subsection{Huffman Encoding}\label{ss:huffman_encoding}
Huffman encoding achieves minimum redundancy of data by giving characters of data variable lengths \cite{huffman}. The lengths of each encoding are determined by how frequently it appears in the data being compressed. Frequently-occurring data are assigned shorter encodings, and infrequently-occurring data are assigned longer encodings. In this way, data can be optimally encoded such that no two characters' representations could be swapped to achieve a shorter encoding result. 

Huffman encoding is done in two passes. In the first pass, the number of occurrences of each data character are counted to determine their relative frequencies. Afterwards, these frequencies are used to construct a binary tree where each data character is a leaf node. The construction of this tree starts from the two least frequent characters. Every time two nodes are combined under a new parent node, the parent node's frequency becomes the sum of the frequencies of its children. This continues until a single root node encompasses the entire tree.

Once the tree is constructed, the lengths and encodings of the characters can be determined. The number of bits used to represent a character is equal to its depth in the tree from the root node. The encoding of each character is determined by the traversal steps required to reach it from the root node. Every left path in the tree concatenates a "0" to the binary encoding of the character, and each right path concatenates a "1" to the binary encoding.

Finally, once each of the leaf nodes has been given an encoding, the second pass can encode the data. For each character of the input data, the second pass looks up its new encoding and outputs this new encoding. This continues until every character of the input has been encoded.

To decode Huffman-encoded data, only the original Huffman tree is needed. Then, the decompressor can simply traverse the Huffman tree for each bit in the encoded data, stopping and starting from the root node every time a leaf is reached. In many cases, the original Huffman tree is stored in canonical Huffman form to save space, but because this canonical form is not used for the hardware compressor and decompressor of this work, it is not relevant.

\subsection{LZ77}\label{ss:lz77}
LZ77-based compression techniques use an algorithm pioneered by Abraham Lempel and Jacob Ziv in 1977 \cite{lz77}. When patterns of characters are repeated in data, LZ77 techniques use references to previous instances of the patterns to represent the data more efficiently. These references are bounded in a sliding window of the most recent data to pass through the compressor.

For LZ77 compression, a sliding window of a fixed size stores the past history of the compressor. As characters are fed into the compressor, the compressor checks to see if each sequence of characters it receives already exists in the sliding window. If it does, instead of representing the characters as literal characters, it represents them as a pointer-length pair. This pair contains both a pointer to the starting position of a character sequence in the sliding window and the length of the matching character sequence. In this way, even very large sequences of repeated data can be compressed down to only a few bytes.

When LZ77 data is decompressed, the decompressor also keeps track of a sliding window of the data that has been decompressed. Whenever the decompressor is given a pointer-length pair, the decompressor looks up the pointer and reads a number of characters after it equal to the length of the pair. A benefit of this encoding is that it does not require any metadata to be shared between the compressor and decompressor, they both automatically build up identical histories as they process the data.

\subsection{LZ78}\label{ss:lz78}
LZ78-based compression techniques instead use an algorithm published by Lempel and Ziv in 1978, after their first seminal paper on compression \cite{lz78}. Like LZ77, LZ78 works by representing sequences of characters as references, but instead of representing them as pointer-length pairs, LZ78 represents references as dictionary entries. These dictionary entries are built by the compressor and rebuilt by the decompressor as the data is read, so there is no need to transmit a dictionary separately.

When compressing with LZ78, every sequence of inputs is added to a dictionary. Initially, the dictionary contains as many entries as there are possible unique characters. Then, as data is streamed through the LZ78 compressor, it adds entries to the dictionary. Anytime a sequence of two or more characters is no longer able to be found in the dictionary, it is added to the dictionary, and the sequence of characters starts from the final character in the sequence that caused it to not match any existing entries. This continues for all of the input characters, and the each character output from the dictionary must be the number of bits required to represent the number of entries in the dictionary. This means that as the dictionary gets longer, the length of each output of the compressor also gets larger to allow it to represent all the valid indices in the dictionary.

\subsection{Deflate}\label{ss:deflate}
Deflate \cite{deflate}.


\section{Existing Hardware Compressors and Decompressors}\label{se:existing_hardware_compressors_and_decompressors}
\subsection{Complete DEFLATE-Like Hardware Compressors and Decompressors}\label{ss:complete_deflate-like_hardware_compressors_and_decompressors}


% \section{Software}\label{se:software}
% This section is a discussion of all relevant software compression algorithms based on LZ77, LZ78, Huffman, or a combination of them. Something important to keep in mind is that many specifications and implementations are open-source, so anyone can use them or implement their own versions.

% \subsection{LZ77}\label{ss:lz77_software}
% Here we will cite all of the LZ77-based compression works, including LZ4, LZO, and LZMA.

% \subsection{LZ78}\label{ss:lz78_software}
% Here we will cite all of the LZ78-based compression works, including LZW.

% \subsection{Combinations of Huffman and LZ77}\label{ss:combinations_of_huffman_and_lz77}
% Here, we will cite all of the works based on a combination of huffman and LZ77-based algorithms. This includes Deflate, GZIP, zlib, and Xpress \cite{deflate, gzip, zlib, xpress}.

% \section{Hardware}\label{se:hardware}
% \subsection{Hardware like Deflate}\label{ss:hardware_like_deflate}
% These papers are hardware compression and decompression engines that are similar to our work, using an LZ-based algorithm followed by entropy encoding like Huffman \cite{microsoft, ibm}. \cite{xpress9} performs LZ77, then Huffman on an FPGA, so is very similar to algorithms like Deflate. \cite{fpgamulticoregzip} does GZIP compression on an FPGA which uses LZ77 and Huffman, but they don't say much about how they designed it or what languages they used. \cite{fpgadeflate} implements deflate on an FPGA in a manner that is tightly coupled with the CPU.\cite{aha} implements several different Gzip, Zlib, and LZS compression algorithms in hardware on a PCIe card at speeds of 10, 20, 40, and 80 Gbps. \cite{ribeiro} implements LZ77 hardware acceleration in hardware and uses this to implement Deflate, but uses the fixed, or static, Huffman encoding when doing so. Comparatively, we use dynamic Huffman encoding, which is more complex and should lead to better compression ratios. \cite{hardwarelz4} is a hardware LZ4 accelerator tested in an FPGA and implemented on a 65nm process node chip. \cite{compresso} is similar to our compression in that it compresses memory, but only compresses 64 bytes at a time, so gets very poor compression ratios.

% \subsection{Hardware LZ}\label{ss:hardware_lz}
% These papers are LZ hardware compressors and decompressors \cite{lz77fpga, lzfpga}.  \cite{lzfpga} performs compression, then decompresses and checks against the original data to be sure no errors occurred. \cite{highthroughputdeflate} does hardware Deflate compression and decompression, but their hardware uses high-level synthesis tools, so the description is actually just written in C++. \cite{xmatchpro} appears to be similar to LZ, it's dictionary-based, looks at the history, and has 4-byte patterns.

% \subsection{Compression Only}\label{ss:compression_only}
% This portion of the paper discusses relevant hardware compressors that are not matched by a hardware decompressor of the same algorithm in the same work. This means that, although compression is accelerated, some form of software decompressor is necessary to make use of the compressor design. \cite{fpgahuffmanlz77} implements Huffman compressors and LZ77 compressors in hardware on an FPGA with VHDL.  \cite{gziponachip} is a hardware implementation of Gzip on an FPGA using OpenCL and high-level synthesis tools.

% \cite{lzrw1} implements an LZRW1 compressor in hardware, but the decompressor is implemented in Java. \cite{lzvlsi} implements an ASIC LZ compression chip that is scalable, but has no decompressor. \cite{fpgagzip} This is an fpga-based data compression implementation of GZip, but requires a software decompression implementation.
% % \cite{acceleratingdatacenter} says they do compression, but don't give any details other than that it is part of the way they accelerated the Bing search engine. % This doesn't seem to add anything to our paper because they don't include any details on their implementation

% \subsection{Decompression Only}\label{ss:decompression_only}
% This portion of the paper discusses relevant hardware decompressors. These decompressors all require data to be first compressed with some kind of software compressor before they can be used.

% \cite{deflatedecompression} does hardware Deflate, but only the decompression, not the compression.

\chapter{Proposed Work} \label{ch:proposed_work}
\section{Huffman Hardware Design}\label{se:huffman_hardware_design}
The Huffman encoding hardware was the first implementation designed. This was because the end goal was to have a similar algorithm to Deflate implemented in hardware, but Huffman encoding is the most complex part of Deflate. If Huffman hardware was not feasible in hardware, it would have been better to know early on.

The Huffman hardware has a number of easily-adjusted parameters that are read from a text file when the hardware is generated. These values, along with a typical value for them, are shown in table \ref{tab:huffman-configuration-table}.

\begin{table}[htb]
	\centering
	\caption{Configurable Huffman Parameters and their typical values.}
	\begin{tabular}{|c|c|}
	    \hline
	    Parameter & Typical Value \\
	    \hline
	    characters to compress & 4096 \\
	    \hline
	    bits in a character & 8 \\
	    \hline
	    characters in the Huffman tree & 16 \\
	    \hline
	    maximum allowed bits in a codeword & 10 \\
	    \hline
	    character counting frequency parallelism & 8 \\
	    \hline
	    compressor output encoding parallelism & 8 \\
	    \hline
	    include statistics on compressed data & false \\
	    \hline
	    allow for inputs smaller than the "characters to compress" size & true \\
	    \hline
	    parallel compressor inputs & false \\
	    \hline
	    parallel compressor outputs & false \\
	    \hline
	    parallel decompressor inputs & false \\ 
	    \hline
	    parallel decompressor outputs & false \\
	    \hline
	\end{tabular}
	\label{tab:huffman-configuration-table}
\end{table}

\subsection{Changes to Standard Huffman Compression}\label{ss:changes_to_standard_huffman_compression}
A number of changes have been made to what would be a standard Huffman compression implementation to reduce the area and latency of the design and improve the maximum clock frequency of the design. In this section, the changes and the motivations to make them will be discussed in detail.

Standard software implementations of Huffman compression transmit the encodings in the Huffman tree from the compressor to the decompressor in what is called a "canonical" Huffman tree. This in canonical tree is the information necessary for the decompressor to determine the correct encoding and length for each character used by the compressor. Although it results in a small metadata size, it requires a substantial amount of area for the decompressor to rebuild the Huffman tree and makes it difficult to achieve high clock speeds. For these reasons, the Huffman hardware design uses a plain text encoding of metadata information, with each character requiring the same number of bits for the character itself, its encoding, and the length of its encoding. This results in a significantly simpler decompressor design which is capable of higher clock speeds and smaller area, but also results in worse compression ratios. However, the compression ratios will be improved by later optimizations.

Because each character of the new Huffman tree format takes a significant number of bits to transmit, it makes sense to attempt to reduce the number of characters used. A beneficial feature of Huffman that allows for this is that Huffman takes advantage of a few short, very frequently used characters, and represents the remaining characters that do not occur very frequently with longer bit sequences. Because of this, it was decided to use only the most frequent characters, the number of which is controlled by the "characters in the Huffman tree" parameter. In this way, instead of transmitting 256 encodings for an 8-bit character Huffman hardware implementation, only 16 characters and their encodings need to be transmitted. To accomplish this, if there are more than "characters in the Huffman tree" characters in the input data, the least frequently occurring data is represented by an escape character. When the escape character is seen by the decompressor, the next "bits in a character" bits are taken as a literal character, not an encoding. This allows for a truncated version of the Huffman tree to be used with negligible effects on the compression ratio of the data, greatly reducing the size of the metadata and improving the overall compression ratio when the metadata is included.

An additional measure to shrink the size of the metadata is called tree normalization. When creating a Huffman tree, the number of characters in the tree is also the worst-case tree depth. both to reduce metadata and to reduce the hardware needed in the compressor and decompressor by reducing the maximum length of an encoding, the worst-case tree depth can be reduced by tree normalization. When normalizing a Huffman tree, any characters that are deeper than depth "maximum allowed bits in a codeword" are instead set to a depth of "maximum allowed bits in a codeword". However, because the amount of space in the encoding space for Huffman compression is equal to the two to the exponent of the number of bits in the longest codeword, and each character takes up two to the exponent of the the maximum number of bits in an encoding minus the number of bits in its encoding, this method can cause there to be less space than is necessary in the encoding space. When this occurs, characters that take up fewer bits are forced to be represented by more bits, freeing up encodings in the encoding space, until there is room for every character to be encoded.

Finally, although most of the Huffman compression and decompression algorithm is sequential, there are two parts that can be done in parallel: counting character frequencies and encoding or decoding the data. To allow for lower latency designs at a higher area cost, both of these components of the design have been individually parameterized so that their latencies can be indirectly controlled.

\subsection{Compressor}\label{ss:huffman_hardware_compressor}
Huffman encoding is the most complicated of the hardware accelerators implemented in this work. The compressor consists of multiple modules, each representing one stage of Huffman compressor, all connected with ready/valid interfaces. The design consists of a character frequency counter, a character frequency sort, a tree generator, a tree depth annotation traversal, a sort for the depths of characters in the tree, a character depth normalizer, a codeword generator, and a compressed data output.

\subsubsection{Character Frequency Counter}\label{sss:character_frequency_counter}
The character frequency counter takes as input a number of characters determined by the "character counting frequency parallelism" parameter. This module iterates through the entire input data, counting how many occurrences of each possible character value there are, then passes an array of the total occurrences for each character to the character frequency sort.

\subsubsection{Character Frequency Sort}\label{sss:character_frequency_sort}
The character frequency sort module sorts the frequencies it is given, but requires two passes to perform the sort. This is because the hardware design will use a limited number of characters in the Huffman tree. After the first pass orders the most commonly occurring "characters in the Huffman tree" characters, the escape codeword is created, with a number of occurrences equal to the sum of all the least commonly occurring characters that are not able to be represented in the Huffman tree. This codeword and its frequency are used in a second pass to make sure that the escape codeword is receives a proper position in the Huffman tree. The order and frequency of each of the characters in the Huffman tree is passed to the next stage.

\subsubsection{Tree Generator}\label{sss:tree_generator}


\subsubsection{Tree Depth Annotation Traversal}\label{sss:tree_depth_counter}

\subsubsection{Character Depth Sort}\label{sss:sort}

\subsubsection{Character Depth Normalizer}\label{sss:tree_normalizer}

\subsubsection{Codeword Generator}\label{sss:codeword_generator}

\subsubsection{Compressed Data Output}\label{sss:compressor_output}

\subsection{Decompressor}\label{ss:huffman_hardware_decompressor}

\section{LZW Hardware Design}\label{se:lzw_hardware_design}
Once the Huffman encoding hardware was complete, it was determined that an LZ-based algorithm was needed to go along with it and improve its compression ratios on the workloads being investigated. The LZ77 was determined to be complicated and expensive to implement in hardware due to the size of the character history window, so LZW, an algorithm specifically designed for implementation in hardware, was chosen.

\subsection{Compressor}\label{ss:lzw_hardware_compressor}
\subsection{Decompressor}\label{ss:lzw_hardware_decompressor}

\section{LZ77 Hardware Design}\label{se:lz77_hardware_design}
After the LZW hardware was completed, it was determined to be a poor fit for Huffman compression due to Huffman's reliance on a small number of frequently used characters and LZW's reliance on generating a large variety of characters that be used very infrequently. For this reason, it was decided that LZ77 would be implemented as well. 

\subsection{Compressor}\label{ss:lz77_hardware_compressor}
\subsection{Decompressor}\label{ss:lz78_hardware_decompressor}

\chapter{Results} \label{ch:results}
\chapter{Discussion} \label{ch:discussion}
\chapter{Conclusions} \label{ch:conclusions}
\chapter{Summary} \label{ch:summary}

	% This is the standard bibtex file. Do not include the .bib extension in <bib_file_name>.
	% Uncomment the following lines to include your bibliography: 
	\bibliography{related_works}
	\bibliographystyle{plainnat}   

	% This formats the chapter name to appendix to properly define the headers:
	\appendix

	% Add your appendices here. You must leave the appendices enclosed in the appendices environment in order for the table of contents to be correct.
	\begin{appendices}
		\chapter{First Appendix} \label{app:appendix_one}
			\section{Section one} \label{ase:app_one_sect_1}
			\section{Section two} \label{ase:app_one_sect_2}
		\chapter{Second Appendix} \label{app:appendix_two}
	\end{appendices}

\end{document}


%****************************************************************************
% Below are some general suggestions for writing your dissertation:
%
% 1. Label everything with a meaningful prefix so that you
%    can refer back to sections, tables, figures, equations, etc.
%    Usage \label{<prefix>:<label_name>} where some suggested
%    prefixes are:
%			ch: Chapter
%     		se: Section
%     		ss: Subsection
%     		sss: Sub-subsection
%			app: Appendix
%     		ase: Appendix section
%     		tab: Tables
%     		fig: Figures
%     		sfig: Sub-figures
%     		eq: Equations
%
% 2. The VTthesis class provides for natbib citations. You should upload
%	 one or more *.bib bibtex files. Suppose you have two bib files: some_refs.bib and 
%    other_refs.bib.  Then your bibliography line to include them
%    will be:
%      \bibliography{some_refs, other_refs}
%    where multiple files are separated by commas. In the body of 
%    your work, you can cite your references using natbib citations.
%    Examples:
%      Citation                     Output
%      -------------------------------------------------------
%      \cite{doe_title_2016}        [18]
%      \citet{doe_title_2016}       Doe et al. [18]
%      \citet*{doe_title_2016}      Doe, Jones, and Smith [18]
%
%    For a complete list of options, see
%      https://www.ctan.org/pkg/natbib?lang=en
%
% 3. Here is a sample table. Notice that the caption is centered at the top. Also
%    notice that we use booktabs formatting. You should not use vertical lines
%    in your tables.
% 
%				\begin{table}[htb]
%					\centering
%					\caption{Approximate computation times in hh:mm:ss for full order 						versus reduced order models.}
%					\begin{tabular}{ccc}
%						\toprule
%						& \multicolumn{2}{c}{Computation Time}\\
%						\cmidrule(r){2-3}
%						$\overline{U}_{in}$ m/s & Full Model & ROM \\
%						\midrule
%						0.90 & 2:00:00 & 2:08:00\\
%						0.88 & 2:00:00 & 0:00:03\\
%						0.92 & 2:00:00 & 0:00:03\\
%						\midrule
%						Total & 6:00:00 & 2:08:06\\
%						\bottomrule
%					\end{tabular}
%					\label{tab:time_rom}
%				\end{table}
% 
% 4. Below are some sample figures. Notice the caption is centered below the
%    figure.
%    a. Single centered figure:
%					\begin{figure}[htb]
%						\centering
%						\includegraphics[scale=0.5]{my_figure.eps}
%						\caption{Average outlet velocity magnitude given an average  
%				        input velocity magnitude of 0.88 m/s.} 
%						\label{fig:output_rom}
%					\end{figure}
%    b. Two by two grid of figures with subcaptions
%					\begin{figure}[htb]
%						\centering
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_1_1.eps}
%							\caption{Subcaption number one}
%							\label{sfig:first_subfig}
%						\end{subfigure}
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_1_2.png}
%							\caption{Subcaption number two}
%							\label{sfig:second_subfig}
%						\end{subfigure}
%
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_2_1.pdf}
%							\caption{Subcaption number three}
%							\label{sfig:third_subfig}
%						\end{subfigure}
%						\begin{subfigure}[h]{0.45\textwidth}
%							\centering
%							\includegraphics[scale=0.4]{figure_2_2.eps}
%							\caption{Subcaption number four}
%							\label{sfig:fourth_subfig}
%						\end{subfigure}
%						\caption{Here is my main caption describing the relationship between the 4 subimages}
%						\label{fig:main_figure}
%					\end{figure}
%
%----------------------------------------------------------------------------
%
% The following is a list of definitions and packages provided by VTthesis:
%
% A. The following packages are provided by the VTthesis class:
%      amsmath, amsthm, amssymb, enumerate, natbib, hyperref, graphicx, 
%      tikz (with shapes and arrows libraries), caption, subcaption,
%      listings, verbatim
%
% B. The following theorem environments are defined by VTthesis:
%      theorem, proposition, lemma, corollary, conjecture
% 
% C. The following definition environments are defined by VTthesis:
%      definition, example, remark, algorithm
%
%----------------------------------------------------------------------------
%
%  I hope this template file and the VTthesis class will keep you from having 
%  to worry about the formatting and allow you to focus on the actual writing.
%  Good luck, and happy writing.
%    Alan Lattimer, VT, 2016
%
%****************************************************************************





